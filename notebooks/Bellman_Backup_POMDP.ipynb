{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17545cf2",
   "metadata": {},
   "source": [
    "***Pattern Description: Missing Recursive Bellman Backup in POMDP-like Problems\n",
    "Overview***\n",
    "\n",
    "This pattern arises in LLM-generated code for decision-making problems modeled as Partially Observable Markov Decision Processes (POMDPs) or similar belief-state frameworks. The core issue is that the LLM fails to implement the recursive Bellman backup operation over belief states, which is essential for correct value iteration or policy computation in these domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d28f65",
   "metadata": {},
   "source": [
    "What is the Recursive Bellman Backup?\n",
    "The Bellman backup is a fundamental dynamic programming step that computes the value function for a belief state by:\n",
    "\n",
    "    Considering all possible actions,\n",
    "\n",
    "    For each action, considering all possible observations,\n",
    "\n",
    "    Recursively backing up the value of the next belief state after the action-observation pair,\n",
    "\n",
    "    Weighting by probabilities to compute expected future value,\n",
    "\n",
    "    Adding immediate expected rewards,\n",
    "\n",
    "    And taking the maximum over actions.\n",
    "\n",
    "This process is inherently recursive and expectation-based over the continuous belief space, which must be discretized or approximated for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f0df3",
   "metadata": {},
   "source": [
    "Pattern Characteristics\n",
    "1. Superficial Correctness:\n",
    "The generated code often includes:\n",
    "\n",
    "State, action, observation, and reward definitions,\n",
    "\n",
    "Functions for belief updates,\n",
    "\n",
    "Loops iterating over discretized beliefs and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc61f3",
   "metadata": {},
   "source": [
    "2. Missing Core Recursion:\n",
    "However, the code:\n",
    "\n",
    "Does not recursively back up future values by iterating over possible observations and updating beliefs accordingly,\n",
    "\n",
    "Instead, it often applies a single-step or simplified update without expectation over observations,\n",
    "\n",
    "Or incorrectly computes expected value without propagating the value function to next belief states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1d977",
   "metadata": {},
   "source": [
    "3. Incorrect or Incomplete Value Iteration:\n",
    "\n",
    "Value updates lack the nested summations over observations and next beliefs,\n",
    "\n",
    "Policies derived from such code are suboptimal or invalid,\n",
    "\n",
    "Resulting test cases fail to reflect correct POMDP behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028091aa",
   "metadata": {},
   "source": [
    "Problem 1:\n",
    "\n",
    "Problem Description\n",
    "This problem models a simplified medical diagnosis scenario using a POMDP framework. The system:\n",
    "\n",
    "Maintains a belief state, representing the probability distribution over three possible hidden health states:\n",
    "['Healthy', 'Sick', 'Critical'].\n",
    "\n",
    "Allows three possible actions:\n",
    "['Wait', 'Test', 'Treat'].\n",
    "\n",
    "Receives noisy observations about the health state, such as:\n",
    "['Positive', 'Negative', 'Uncertain'].\n",
    "\n",
    "Has transition probabilities, observation models, and a reward structure based on medical outcomes and actions.\n",
    "\n",
    "The objective is to implement a value iteration algorithm over discretized belief states to compute:\n",
    "\n",
    "✅ The optimal value function for each belief.\n",
    "✅ The corresponding optimal policy, mapping beliefs to actions.\n",
    "\n",
    "The correct solution requires a recursive Bellman backup over belief states, considering all possible actions and observations, updating beliefs accordingly, and propagating future value estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e3ab4",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "\n",
    "Task:\n",
    "Implement value iteration for a simple POMDP modeling medical decision-making.\n",
    "There are 3 health states: 'Healthy', 'Sick', 'Critical'.\n",
    "The agent can 'Wait', 'Test', or 'Treat'.\n",
    "Observations can be 'Positive', 'Negative', 'Uncertain', based on a noisy observation model.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Define transition probabilities and observation models.\n",
    "\n",
    "Implement belief updates based on actions and observations.\n",
    "\n",
    "Discretize the belief space.\n",
    "\n",
    "Run value iteration to compute the optimal policy and value function over beliefs.\n",
    "\n",
    "The code should:\n",
    "\n",
    "Track value estimates for each belief.\n",
    "\n",
    "Update beliefs based on observations.\n",
    "\n",
    "Iterate to convergence.\n",
    "\n",
    "Generate an example at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d8a342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belief: [0. 0. 1.], Action: Treat, Value: 1.0\n",
      "Belief: [0.   0.11 0.89], Action: Treat, Value: 1.11\n",
      "Belief: [0.   0.22 0.78], Action: Treat, Value: 1.22\n",
      "Belief: [0.   0.33 0.67], Action: Treat, Value: 1.33\n",
      "Belief: [0.   0.44 0.56], Action: Treat, Value: 1.44\n",
      "Belief: [0.   0.56 0.44], Action: Treat, Value: 1.56\n",
      "Belief: [0.   0.67 0.33], Action: Treat, Value: 1.67\n",
      "Belief: [0.   0.78 0.22], Action: Treat, Value: 1.78\n",
      "Belief: [0.   0.89 0.11], Action: Treat, Value: 1.89\n",
      "Belief: [0. 1. 0.], Action: Treat, Value: 2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# State, Action, Observation spaces\n",
    "states = ['Healthy', 'Sick', 'Critical']\n",
    "actions = ['Wait', 'Test', 'Treat']\n",
    "observations = ['Positive', 'Negative', 'Uncertain']\n",
    "\n",
    "num_states = len(states)\n",
    "\n",
    "# Transition model: T[s][a][s']\n",
    "T = {\n",
    "    'Healthy': {\n",
    "        'Wait': {'Healthy': 0.9, 'Sick': 0.1, 'Critical': 0.0},\n",
    "        'Test': {'Healthy': 1.0},\n",
    "        'Treat': {'Healthy': 1.0}\n",
    "    },\n",
    "    'Sick': {\n",
    "        'Wait': {'Sick': 0.6, 'Critical': 0.3, 'Healthy': 0.1},\n",
    "        'Test': {'Sick': 1.0},\n",
    "        'Treat': {'Healthy': 0.6, 'Sick': 0.3, 'Critical': 0.1}\n",
    "    },\n",
    "    'Critical': {\n",
    "        'Wait': {'Critical': 0.9, 'Sick': 0.1},\n",
    "        'Test': {'Critical': 1.0},\n",
    "        'Treat': {'Sick': 0.2, 'Critical': 0.8}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Observation model: O[a][s'][o]\n",
    "O = {\n",
    "    'Test': {\n",
    "        'Healthy': {'Negative': 0.9, 'Positive': 0.05, 'Uncertain': 0.05},\n",
    "        'Sick':    {'Negative': 0.2, 'Positive': 0.7,  'Uncertain': 0.1},\n",
    "        'Critical':{'Negative': 0.05, 'Positive': 0.9, 'Uncertain': 0.05},\n",
    "    },\n",
    "    'Wait': {\n",
    "        s: {'Uncertain': 1.0} for s in states\n",
    "    },\n",
    "    'Treat': {\n",
    "        s: {'Uncertain': 1.0} for s in states\n",
    "    }\n",
    "}\n",
    "\n",
    "# Rewards: R[s][a]\n",
    "R = {\n",
    "    'Healthy':  {'Wait': 1, 'Test': -1, 'Treat': -2},\n",
    "    'Sick':     {'Wait': -1, 'Test': -1, 'Treat': 2},\n",
    "    'Critical': {'Wait': -3, 'Test': -2, 'Treat': 1}\n",
    "}\n",
    "\n",
    "# Discretize belief space\n",
    "def generate_beliefs(num_points=10):\n",
    "    grid = np.linspace(0, 1, num_points)\n",
    "    beliefs = [b for b in itertools.product(grid, repeat=num_states)\n",
    "               if np.isclose(sum(b), 1)]\n",
    "    return [np.array(b) for b in beliefs]\n",
    "\n",
    "# Belief update\n",
    "def update_belief(b, a, o):\n",
    "    new_b = np.zeros(num_states)\n",
    "    for i, s_prime in enumerate(states):\n",
    "        total = 0\n",
    "        for j, s in enumerate(states):\n",
    "            trans = T[s][a].get(s_prime, 0)\n",
    "            total += b[j] * trans\n",
    "        obs_prob = O[a][s_prime].get(o, 0)\n",
    "        new_b[i] = total * obs_prob\n",
    "    new_b /= np.sum(new_b) + 1e-8\n",
    "    return new_b\n",
    "\n",
    "# Value iteration\n",
    "def value_iteration(beliefs, gamma=0.95, max_iter=100, tol=1e-3):\n",
    "    V = {tuple(b): 0 for b in beliefs}\n",
    "    policy = {}\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        new_V = {}\n",
    "        delta = 0\n",
    "\n",
    "        for b in beliefs:\n",
    "            best_val = -np.inf\n",
    "            best_act = None\n",
    "\n",
    "            for a in actions:\n",
    "                val = 0\n",
    "                for s_idx, s in enumerate(states):\n",
    "                    val += b[s_idx] * R[s][a]\n",
    "\n",
    "                exp_future_val = 0\n",
    "                for o in observations:\n",
    "                    b_next = update_belief(b, a, o)\n",
    "                    b_next_key = tuple(np.round(b_next, 3))\n",
    "                    if b_next_key in V:\n",
    "                        exp_future_val += np.sum([\n",
    "                            b[s_idx] * T[states[s_idx]][a].get(states[i], 0) *\n",
    "                            O[a][states[i]].get(o, 0) * V[b_next_key]\n",
    "                            for i in range(num_states)\n",
    "                            for s_idx in range(num_states)\n",
    "                        ])\n",
    "\n",
    "                val += gamma * exp_future_val\n",
    "\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_act = a\n",
    "\n",
    "            new_V[tuple(b)] = best_val\n",
    "            policy[tuple(b)] = best_act\n",
    "            delta = max(delta, abs(V[tuple(b)] - best_val))\n",
    "\n",
    "        V = new_V\n",
    "        if delta < tol:\n",
    "            break\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "# 🔁 Run everything\n",
    "beliefs = generate_beliefs()\n",
    "V, policy = value_iteration(beliefs)\n",
    "\n",
    "# Sample output\n",
    "for b in beliefs[:10]:\n",
    "    print(f\"Belief: {np.round(b,2)}, Action: {policy[tuple(b)]}, Value: {round(V[tuple(b)],2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58ed75d",
   "metadata": {},
   "source": [
    "Exactly — the issue is with how the Bellman backup’s expected future value is calculated in your code.\n",
    "\n",
    "In your current implementation, you’re mixing summations over both current and next states inside the expected future value in a way that inflates the result incorrectly. More specifically:\n",
    "\n",
    "\n",
    "Your code tries to sum over states and multiply by the value of the same updated belief multiple times, which distorts the expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7ec303",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "Model an inventory management POMDP with partial observations:\n",
    "\n",
    "- Inventory level: 0 to 5 units.\n",
    "- Actions: order 0, 1, or 2 units.\n",
    "- Demand is unknown, either High or Low demand with probabilities.\n",
    "- Only partial observation of demand via sales data (censored).\n",
    "- Reward is profit from sales minus ordering cost.\n",
    "- Belief state is distribution over demand levels.\n",
    "- Implement belief update from sales observations.\n",
    "- Implement recursive value iteration for policy optimization with gamma=0.95.\n",
    "- Run value iteration for 20 iterations.\n",
    "\n",
    "Include:\n",
    "- State, action, observation definitions.\n",
    "- Transition, observation probabilities.\n",
    "- Belief update.\n",
    "- Recursive value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72487e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import poisson\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Parameters\n",
    "inventory_levels = range(6)\n",
    "actions = [0, 1, 2]\n",
    "observations = range(6)\n",
    "demand_types = ['Low', 'High']\n",
    "demand_probs = {'Low': 0.6, 'High': 0.4}\n",
    "demand_dists = {'Low': poisson(1), 'High': poisson(3)}\n",
    "gamma = 0.95\n",
    "reward_per_unit = 10\n",
    "cost_per_unit = 2\n",
    "\n",
    "# Belief grid (discretized)\n",
    "belief_grid = np.linspace(0, 1, 11)  # P(Low), so P(High) = 1 - p\n",
    "\n",
    "def expected_demand(type_):\n",
    "    return demand_dists[type_].mean()\n",
    "\n",
    "def transition(s, a, demand):\n",
    "    return max(0, min(s + a - demand, 5))\n",
    "\n",
    "def observation_prob(obs, s, a, demand_type):\n",
    "    inv = s + a\n",
    "    return sum(\n",
    "        demand_dists[demand_type].pmf(d) \n",
    "        for d in range(20) if min(d, inv) == obs\n",
    "    )\n",
    "\n",
    "def belief_update(belief, obs, s, a):\n",
    "    inv = s + a\n",
    "    numerators = {}\n",
    "    for d_type in demand_types:\n",
    "        prob = 0\n",
    "        for d in range(20):\n",
    "            if min(d, inv) == obs:\n",
    "                prob += demand_dists[d_type].pmf(d)\n",
    "        numerators[d_type] = prob * belief[d_type]\n",
    "    total = sum(numerators.values())\n",
    "    if total == 0:\n",
    "        return belief  # No update possible\n",
    "    return {k: v / total for k, v in numerators.items()}\n",
    "\n",
    "def expected_reward(s, a, belief):\n",
    "    inv = s + a\n",
    "    reward = 0\n",
    "    for d_type in demand_types:\n",
    "        prob = belief[d_type]\n",
    "        for d in range(20):\n",
    "            p_d = demand_dists[d_type].pmf(d)\n",
    "            sold = min(d, inv)\n",
    "            reward += prob * p_d * (sold * reward_per_unit - a * cost_per_unit)\n",
    "    return reward\n",
    "\n",
    "# Initialize value function\n",
    "V = defaultdict(lambda: np.zeros(len(inventory_levels)))\n",
    "policy = defaultdict(lambda: np.zeros(len(inventory_levels), dtype=int))\n",
    "\n",
    "# Value Iteration\n",
    "for it in range(20):\n",
    "    new_V = defaultdict(lambda: np.zeros(len(inventory_levels)))\n",
    "    for b_p in belief_grid:\n",
    "        belief = {'Low': b_p, 'High': 1 - b_p}\n",
    "        for s in inventory_levels:\n",
    "            best_val = -np.inf\n",
    "            best_act = 0\n",
    "            for a in actions:\n",
    "                inv = s + a\n",
    "                exp_val = expected_reward(s, a, belief)\n",
    "                for obs in observations:\n",
    "                    # Belief update\n",
    "                    b_prime = belief_update(belief, obs, s, a)\n",
    "                    b_prime_key = round(b_prime['Low'], 1)\n",
    "                    # Transition\n",
    "                    val = 0\n",
    "                    for d_type in demand_types:\n",
    "                        for d in range(20):\n",
    "                            if min(d, inv) == obs:\n",
    "                                p_d = demand_dists[d_type].pmf(d)\n",
    "                                s_prime = transition(s, a, d)\n",
    "                                val += belief[d_type] * p_d * V[b_prime_key][s_prime]\n",
    "                    exp_val += gamma * val\n",
    "                if exp_val > best_val:\n",
    "                    best_val = exp_val\n",
    "                    best_act = a\n",
    "            new_V[round(b_p, 1)][s] = best_val\n",
    "            policy[round(b_p, 1)][s] = best_act\n",
    "    V = new_V\n",
    "\n",
    "# Display optimal policy\n",
    "print(\"Optimal Policy (belief in Low demand vs inventory level):\")\n",
    "for b in belief_grid:\n",
    "    b_key = round(b, 1)\n",
    "    print(f\"Belief Low={b_key:.1f}: \", policy[b_key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c8d9be",
   "metadata": {},
   "source": [
    "Summary:\n",
    "The code roughly implements Bellman backup logic but misses weighting the expected future value by the observation probability \n",
    "\n",
    "This leads to overcounting the future values because it adds the unnormalized sums for each observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a2103",
   "metadata": {},
   "source": [
    "Implement a model for sensor maintenance with noisy observations\n",
    "\n",
    "Requirements:\n",
    "\n",
    "- **Hidden States**:\n",
    "    - 0 = Failed\n",
    "    - 1 = Degraded\n",
    "    - 2 = Healthy\n",
    "\n",
    "- **Actions**:\n",
    "    - 0 = Do nothing\n",
    "    - 1 = Send maintenance\n",
    "    - 2 = Replace sensor (restores to Healthy)\n",
    "\n",
    "- **Observations**:\n",
    "    - \"Bad\", \"Uncertain\", \"Good\"\n",
    "\n",
    "- **Transition Model (P(s' | s, a))**:\n",
    "    - If action = 0 (Do nothing):\n",
    "        - Healthy → Degraded (10% chance)\n",
    "        - Degraded → Failed (20% chance)\n",
    "        - Failed remains Failed\n",
    "    - If action = 1 (Maintenance):\n",
    "        - Degraded → Healthy (50% chance), otherwise unchanged\n",
    "        - Failed → Degraded (30% chance), otherwise unchanged\n",
    "    - If action = 2 (Replace):\n",
    "        - Always transitions to Healthy\n",
    "\n",
    "- **Observation Model (P(o | s))**:\n",
    "    - If state = Failed:\n",
    "        - \"Bad\" = 0.9, \"Uncertain\" = 0.1, \"Good\" = 0.0\n",
    "    - If state = Degraded:\n",
    "        - \"Bad\" = 0.3, \"Uncertain\" = 0.5, \"Good\" = 0.2\n",
    "    - If state = Healthy:\n",
    "        - \"Bad\" = 0.0, \"Uncertain\" = 0.2, \"Good\" = 0.8\n",
    "\n",
    "- **Rewards**:\n",
    "    - Large penalty for being in Failed state\n",
    "    - Cost for maintenance and replacement\n",
    "    - Bonus for Healthy operation\n",
    "\n",
    "- **Belief Update**:\n",
    "    - Maintain belief vector over hidden states\n",
    "    - After each observation, update belief using Bayes' rule\n",
    "\n",
    "- **Value Iteration**:\n",
    "    - For each belief state and action:\n",
    "            - Expectation over hidden states and observations\n",
    "            - Update belief based on noisy observation\n",
    "            - Recursively compute future value with discount factor (e.g., 0.9)\n",
    "    - Run for fixed number of iterations (e.g., 20)\n",
    "    - Store optimal policy mapping from belief to action\n",
    "\n",
    "\n",
    "Goal:\n",
    "- Find optimal sensor maintenance policy under partial observability and observation noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff5bdc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended action: DoNothing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9174/4162913785.py:50: RuntimeWarning: invalid value encountered in divide\n",
      "  return new_belief / np.sum(new_belief)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# 📌 Model Definitions\n",
    "states = ['Failed', 'Degraded', 'Healthy']\n",
    "actions = ['DoNothing', 'Maintenance', 'Replace']\n",
    "observations = ['Bad', 'Uncertain', 'Good']\n",
    "discount = 0.9\n",
    "\n",
    "# Transition model P(s' | s, a)\n",
    "T = {\n",
    "    0: np.array([  # Do nothing\n",
    "        [1.0,   0.0,   0.0],\n",
    "        [0.2,   0.8,   0.0],\n",
    "        [0.0,   0.1,   0.9]\n",
    "    ]),\n",
    "    1: np.array([  # Maintenance\n",
    "        [0.7,   0.3,   0.0],\n",
    "        [0.0,   0.5,   0.5],\n",
    "        [0.0,   0.0,   1.0]\n",
    "    ]),\n",
    "    2: np.array([  # Replace\n",
    "        [0.0,   0.0,   1.0],\n",
    "        [0.0,   0.0,   1.0],\n",
    "        [0.0,   0.0,   1.0]\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Observation model P(o | s)\n",
    "O = np.array([\n",
    "    [0.9, 0.1, 0.0],  # \"Bad\" for Failed, Degraded, Healthy\n",
    "    [0.3, 0.5, 0.2],  # \"Uncertain\"\n",
    "    [0.0, 0.2, 0.8]   # \"Good\"\n",
    "])\n",
    "\n",
    "# Reward matrix R[s, a]\n",
    "R = np.array([\n",
    "    [-100, -10, -50],  # Failed\n",
    "    [ -20, -10, -50],  # Degraded\n",
    "    [ +10, -10, -50]   # Healthy\n",
    "])\n",
    "\n",
    "# 🔁 Belief Update Function\n",
    "def update_belief(belief, action, observation):\n",
    "    new_belief = np.zeros(3)\n",
    "    for s_prime in range(3):\n",
    "        obs_prob = O[observation, s_prime]\n",
    "        trans_prob = sum(T[action][s_prime, s] * belief[s] for s in range(3))\n",
    "        new_belief[s_prime] = obs_prob * trans_prob\n",
    "    return new_belief / np.sum(new_belief)\n",
    "\n",
    "# 💰 Value Iteration\n",
    "def value_iteration(num_iterations=20, resolution=11):\n",
    "    belief_points = [\n",
    "        np.array([b1, b2, 1.0 - b1 - b2])\n",
    "        for b1 in np.linspace(0, 1, resolution)\n",
    "        for b2 in np.linspace(0, 1 - b1, resolution)\n",
    "    ]\n",
    "\n",
    "    V = {tuple(b): 0 for b in belief_points}\n",
    "    policy = {}\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        new_V = {}\n",
    "        for b in belief_points:\n",
    "            b = np.array(b)\n",
    "            best_val = -np.inf\n",
    "            best_act = None\n",
    "\n",
    "            for a in range(3):  # All actions\n",
    "                reward = sum(b[s] * R[s, a] for s in range(3))\n",
    "                future = 0\n",
    "                for o in range(3):\n",
    "                    b_new = update_belief(b, a, o)\n",
    "                    b_key = tuple(np.round(b_new, 4))\n",
    "                    future += np.dot(O[o], T[a] @ b) * V.get(b_key, 0)\n",
    "                value = reward + discount * future\n",
    "\n",
    "                if value > best_val:\n",
    "                    best_val = value\n",
    "                    best_act = a\n",
    "\n",
    "            new_V[tuple(b)] = best_val\n",
    "            policy[tuple(b)] = best_act\n",
    "        V = new_V\n",
    "    return policy\n",
    "\n",
    "# 🏁 Example Run\n",
    "if __name__ == \"__main__\":\n",
    "    belief = np.array([0.0, 0.2, 0.8])  # Most likely Healthy\n",
    "    policy = value_iteration()\n",
    "    action_index = policy[tuple(np.round(belief, 4))]\n",
    "    print(f\"Recommended action: {actions[action_index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53feb614",
   "metadata": {},
   "source": [
    "Your future expected value calculation is currently incorrect because the observation probability is miscalculated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
